First of all I would prefer the std::set<std::string> as a main container
rather than std::unordered_map<std::string>, because it's more cache-friendly 
especially for such a big dataset.

-----------------------------------------------------------------------------
Idea #1

For example we have 32GB of text data. The external device (e.g. user)
supplies data partitially. Let's assume that lots of the words are unique.
Let's split the text (32GB) to 8 parts by 4 GB.
Each part we save to its own file. (There are 8 files for each part of the text)
Once we've done that we open 2 files (1st and 2nd) and perform std::set_intersection() algo.
Save results to the file of results. 
Then we open the next 2 files (1st and 3rd) and do the same
In result we get O(n^2) algo. And program takes ~8GB of RAM (since it depends on
size of each file it could be reduced even to several KiB)

Pseudocode:
    for (i_file: files)
        for(j_file: files)
            intersection(j_file, i_file)
    i_file = next(i_file)

After that we open results' files (one by one) and count the number of unique words.

Cons:
O(n^2), lots of disk operations.

Pros:
Due to google.com the number of unique words in a text f(n)
can be estimated to be: 4,1n^0,67. 
So, only 9.18% from 100.000 words are unique.
Therefore there is no need to accomplish the algorithm on 100%


-----------------------------------------------------------------------------
Idea #2

Use a 'Trie' data structure!